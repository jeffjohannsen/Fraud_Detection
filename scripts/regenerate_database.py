#!/usr/bin/env python3
"""
Regenerate the fraud detection database with updated predictions.

This script processes all records from simulate_data.json and generates
fresh predictions using the current trained models. Uses optimized batch
processing for efficiency.

BATCH PROCESSING STRATEGY (3-step approach):
  1. Data Cleaning: Process all 2,868 records through preprocessing pipeline
  2. NLP Feature Engineering: Batch predict fraud probabilities for all text features
     (name, description, org_name, org_desc) using sklearn pipelines
  3. Final Predictions: Score each record with Random Forest and save to database

PERFORMANCE:
  - Total time: ~4-5 minutes for 2,868 records
  - ~5x faster than one-by-one processing
  - Progress indicators show status at each step

USAGE:
  $ python scripts/regenerate_database.py

  Script will:
  - Drop existing fraud_records table
  - Process all simulate_data.json records
  - Display progress and risk distribution statistics

OUTPUT:
  - SQLite database: data/databases/fraud_detection_local.db
  - Statistics: High/Medium/Low risk counts and percentages

WHEN TO RUN:
  - After retraining models (src/pipeline/retrain_models.py)
  - To refresh database with new model versions
  - To rebuild database from scratch

REQUIREMENTS:
  - Trained models in models/ directory (5 pickle files)
  - simulate_data.json in data/processed/
  - Active Python virtual environment
"""

import json
import sys
import time
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import pickle as pkl

from src.config import DATA_PROCESSED, DATABASE_PATH, MODELS_DIR
from src.database.db import LocalDatabase
from src.scoring.predict import EventRecord


def main():
    """
    Main execution function for database regeneration.

    Returns:
        int: Exit code (0 for success, 1 for failure)
    """
    print("\n" + "=" * 70)
    print("DATABASE REGENERATION - Fraud Detection System")
    print("=" * 70)

    start_time = time.time()

    # ========== STEP 1: Load Models ==========
    print("\n[1/4] Loading trained models...")
    models_path = MODELS_DIR

    try:
        # NLP text classifiers for feature engineering
        models = {}
        model_files = {
            "nlp_model_description": "nlp_description_text_clf_pipeline",
            "nlp_model_name": "nlp_name_text_clf_pipeline",
            "nlp_model_org_desc": "nlp_org_desc_text_clf_pipeline",
            "nlp_model_org_name": "nlp_org_name_text_clf_pipeline",
            "final_model": "forest_clf",
        }

        for key, filename in model_files.items():
            model_path = models_path / f"{filename}.pkl"
            if not model_path.exists():
                raise FileNotFoundError(f"Missing model: {model_path}")
            with open(model_path, "rb") as f:
                models[key] = pkl.load(f)

        print(f"      ‚úì Loaded 4 NLP models + 1 Random Forest classifier")

    except FileNotFoundError as e:
        print(f"      ‚úó Model loading failed: {e}")
        print(f"      Run: python src/pipeline/retrain_models.py")
        return 1
    except Exception as e:
        print(f"      ‚úó Unexpected error loading models: {e}")
        return 1

    # ========== STEP 2: Initialize Database ==========
    print("\n[2/4] Initializing database connection...")

    try:
        db = LocalDatabase()  # Uses DATABASE_PATH from config

        # Drop existing table and recreate
        print("      Dropping existing fraud_records table...")
        db.drop_table()
        db.create_table()
        print("      ‚úì Database ready for fresh predictions")

    except Exception as e:
        print(f"      ‚úó Database initialization failed: {e}")
        return 1

    # ========== STEP 3: Load Data ==========
    print("\n[3/4] Loading simulate_data.json (holdout set)...")
    data_path = DATA_PROCESSED / "simulate_data.json"

    if not data_path.exists():
        print(f"      ‚úó Error: {data_path} not found")
        print(f"      Please ensure simulate_data.json exists in data/processed/")
        return 1

    try:
        # Load JSONL format (one JSON object per line)
        with open(data_path) as f:
            records = [json.loads(line) for line in f if line.strip()]
        print(f"      ‚úì Loaded {len(records):,} records")
    except json.JSONDecodeError as e:
        print(f"      ‚úó Invalid JSON in {data_path}: {e}")
        return 1
    except Exception as e:
        print(f"      ‚úó Error loading data: {e}")
        return 1

    # ========== STEP 4: Batch Process All Records ==========
    print(f"\n[4/4] Processing {len(records):,} records...")
    print("      Using 3-step batch approach for optimal performance")

    # Define database schema columns (LocalDatabase has 44 columns)
    schema_columns = [
        "object_id",
        "fraud_proba",
        "record_predicted_datetime",
        "risk_classification",
        "name",
        "description",
        "org_name",
        "org_desc",
        "approx_payout_date",
        "body_length",
        "channels",
        "country",
        "currency",
        "delivery_method",
        "email_domain",
        "event_created",
        "event_end",
        "event_published",
        "event_start",
        "fb_published",
        "gts",
        "has_analytics",
        "has_header",
        "has_logo",
        "listed",
        "name_length",
        "num_order",
        "num_payouts",
        "org_facebook",
        "org_twitter",
        "sale_duration",
        "sale_duration2",
        "show_map",
        "user_age",
        "user_type",
        "venue_latitude",
        "venue_longitude",
        "num_previous_payouts",
        "avg_previous_payout",
        "total_previous_payout",
        "num_ticket_types",
        "avg_ticket_cost",
        "total_ticket_value",
        "total_empty_values",
    ]

    successful = 0
    failed = 0

    # Step 4.1: Clean all records
    print(f"\n      Step 1/3: Cleaning {len(records):,} records...")
    cleaned_records = []
    for i, raw_record in enumerate(records):
        if (i + 1) % 500 == 0:
            print(f"                Cleaned {i + 1:,}/{len(records):,}...")

        try:
            event = EventRecord(raw_record)
            event.clean_record()
            cleaned_records.append(event)
        except Exception as e:
            print(f"      ‚ö†Ô∏è  Error cleaning record {i+1}: {str(e)}")
            failed += 1
            continue

    print(f"      ‚úì Cleaned {len(cleaned_records):,} records successfully")

    # Step 4.2: Batch NLP predictions
    print(f"\n      Step 2/3: Batch NLP feature engineering...")
    text_features = ["name", "description", "org_name", "org_desc"]

    # Build NLP models dict for batch processing
    nlp_models_dict = {
        feature: models[f"nlp_model_{feature}"] for feature in text_features
    }

    for event in cleaned_records:
        event.create_nlp_predictions(nlp_models_dict)

    print(f"      ‚úì Batch NLP completed for {len(text_features)} text features")

    # Step 4.3: Final predictions and database insertion
    print(f"\n      Step 3/3: Final predictions and database insertion...")
    final_model = models["final_model"]

    for i, event in enumerate(cleaned_records):
        try:
            # Get final prediction
            event.predict_record(final_model)

            # Convert to dictionary for database insertion (pandas Series method)
            record_dict = event.processed_record.to_dict()

            # Filter to only include schema columns (prevents schema mismatch)
            filtered_dict = {
                k: v for k, v in record_dict.items() if k in schema_columns
            }

            # Insert into database
            db.insert_record(filtered_dict)
            successful += 1

            if (i + 1) % 500 == 0:
                print(f"                Saved {i + 1:,}/{len(cleaned_records):,}...")

        except Exception as e:
            print(f"      ‚ö†Ô∏è  Error scoring/saving record {i+1}: {str(e)}")
            failed += 1
            continue

    elapsed_time = time.time() - start_time
    minutes = int(elapsed_time // 60)
    seconds = elapsed_time % 60

    # ========== Display Results ==========
    print(f"\n{'=' * 70}")
    print("REGENERATION COMPLETE")
    print(f"{'=' * 70}")
    print(f"\n‚è±Ô∏è  Processing Time: {minutes}m {seconds:.1f}s")
    print(f"‚úì  Successfully scored: {successful:,} records")
    if failed > 0:
        print(f"‚ö†Ô∏è  Failed: {failed:,} records")

    # Display risk distribution statistics
    try:
        risk_stats = db.get_risk_counts()
        print(f"\nüìä Risk Distribution:")
        print(f"   Total records:         {risk_stats['record_count']:>6,}")
        print(
            f"   High risk (‚â•0.10):     {risk_stats['high_count']:>6,}  ({risk_stats['high_perc']:>5.1f}%)"
        )
        print(
            f"   Medium risk (‚â•0.03):   {risk_stats['med_count']:>6,}  ({risk_stats['med_perc']:>5.1f}%)"
        )
        print(
            f"   Low risk (<0.03):      {risk_stats['low_count']:>6,}  ({risk_stats['low_perc']:>5.1f}%)"
        )

        fraud_predictions = risk_stats["high_count"] + risk_stats["med_count"]
        print(f"\n   Dual-threshold flagged: {fraud_predictions:>6,}  (High + Medium)")

    except Exception as e:
        print(f"\n‚ö†Ô∏è  Could not retrieve risk statistics: {e}")

    print(f"\n‚úÖ Database ready at: {DATABASE_PATH}")
    print(f"   Run Flask app: python run_app.py")
    print(f"{'=' * 70}\n")

    return 0


if __name__ == "__main__":
    sys.exit(main())
